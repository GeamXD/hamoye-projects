{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n"
      ],
      "metadata": {
        "id": "z4zHLwmzf2Hm"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "df = pd.read_csv('/content/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n",
        "\n",
        "# Split the data into an 80-20 train-test split with a random state of “1”.\n",
        "\n",
        "df['TotalCharges'] = df['TotalCharges'].replace(' ', '0').astype(float)\n",
        "df['Churn'] = (df['Churn'] == 'Yes').astype(int)\n",
        "\n",
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']\n",
        "\n",
        "# split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "\n",
        "# categorical = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService','OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies','Contract', 'PaperlessBilling', 'PaymentMethod'] numerical = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "categorical = ['gender', 'SeniorCitizen', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines', 'InternetService','OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV', 'StreamingMovies','Contract', 'PaperlessBilling', 'PaymentMethod']\n",
        "numerical = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "X_train = X_train[categorical + numerical]\n",
        "X_test = X_test[categorical + numerical]\n",
        "\n",
        "\n",
        "# Scale the numerical features\n",
        "scaler = StandardScaler()\n",
        "X_train_numerical = scaler.fit_transform(X_train[numerical])\n",
        "X_test_numerical = scaler.transform(X_test[numerical])\n",
        "\n",
        "# One-hot encode the categorical features\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "X_train_categorical = encoder.fit_transform(X_train[categorical])\n",
        "X_test_categorical = encoder.transform(X_test[categorical])\n",
        "\n",
        "# Convert the scaled and encoded features back to dataframes and put back the column names\n",
        "X_train_scaled = pd.DataFrame(X_train_numerical, columns=numerical)\n",
        "X_test_scaled = pd.DataFrame(X_test_numerical, columns=numerical)\n",
        "\n",
        "X_train_encoded = pd.DataFrame(X_train_categorical, columns=encoder.get_feature_names_out())\n",
        "X_test_encoded = pd.DataFrame(X_test_categorical, columns=encoder.get_feature_names_out())\n",
        "\n",
        "\n",
        "\n",
        "# Use scikit learn to train a random forest and extra trees classifier, and\n",
        "# use xgboost and lightgbm to train an extreme boosting model and a light\n",
        "# gradient boosting model. Use random_state = 1 for training all models\n",
        "# and evaluate on the test set\n",
        "\n",
        "# Combine scaled numerical and one-hot encoded categorical features into train and test set dataframes\n",
        "X_train_combined = pd.concat([X_train_scaled, X_train_encoded], axis=1)\n",
        "X_test_combined = pd.concat([X_test_scaled, X_test_encoded], axis=1)\n",
        "\n",
        "# Train the models\n",
        "models = [\n",
        "    RandomForestClassifier(random_state=1),\n",
        "    ExtraTreesClassifier(random_state=1),\n",
        "    XGBClassifier(random_state=1),\n",
        "    LGBMClassifier(random_state=1)\n",
        "]\n",
        "\n",
        "for model in models:\n",
        "    model.fit(X_train_combined, y_train)\n",
        "    score = model.score(X_test_combined, y_test)\n",
        "    print(f\"{model.__class__.__name__}: {score}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDVOiebGgASg",
        "outputId": "58f458a1-eaab-470a-9ee6-1a61784b75e0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RandomForestClassifier: 0.7913413768630234\n",
            "ExtraTreesClassifier: 0.7672107877927609\n",
            "XGBClassifier: 0.7934705464868701\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 1521, number of negative: 4113\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001005 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 670\n",
            "[LightGBM] [Info] Number of data points in the train set: 5634, number of used features: 46\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.269968 -> initscore=-0.994785\n",
            "[LightGBM] [Info] Start training from score -0.994785\n",
            "LGBMClassifier: 0.8034066713981547\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the hyperparameter grid\n",
        "n_estimators = [50, 100, 300, 500, 1000]\n",
        "min_samples_split = [2, 3, 5, 7, 9]\n",
        "min_samples_leaf = [1, 2, 4, 6, 8]\n",
        "max_features = ['auto', 'sqrt', 'log2', None]\n",
        "\n",
        "hyperparameter_grid = {\n",
        "    'n_estimators': n_estimators,\n",
        "    'min_samples_leaf': min_samples_leaf,\n",
        "    'min_samples_split': min_samples_split,\n",
        "    'max_features': max_features\n",
        "}\n",
        "\n",
        "# Initialize the ExtraTreesClassifier\n",
        "et_clf = ExtraTreesClassifier(random_state=1)\n",
        "\n",
        "# Perform randomized search with cross-validation\n",
        "random_search = RandomizedSearchCV(estimator=et_clf, param_distributions=hyperparameter_grid, cv=5, n_iter=10, scoring='accuracy', n_jobs=-1, verbose=1, random_state=1)\n",
        "\n",
        "# Fit the model\n",
        "random_search.fit(X_train_combined, y_train)\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_params = random_search.best_params_\n",
        "\n",
        "# Print the best hyperparameters\n",
        "print(\"Best hyperparameters:\", best_params)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLGKnESFmkoT",
        "outputId": "83b5c92e-953a-4d66-db25-d6220854df4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new ExtraTreesClassifier with the best hyperparameters\n",
        "et_clf_tuned = ExtraTreesClassifier(random_state=1, **best_params)\n",
        "\n",
        "# Train the new model\n",
        "et_clf_tuned.fit(X_train_combined, y_train)\n",
        "\n",
        "# Calculate the accuracy of the new model\n",
        "et_clf_tuned_score = et_clf_tuned.score(X_test_combined, y_test)\n",
        "\n",
        "# Calculate the accuracy of the initial model\n",
        "et_clf_initial_score = random_search.cv_results_['mean_test_score'][0]\n",
        "\n",
        "# Print the accuracy of both models\n",
        "print(f\"Initial ExtraTreesClassifier accuracy: {et_clf_initial_score}\")\n",
        "print(f\"Tuned ExtraTreesClassifier accuracy: {et_clf_tuned_score}\")\n",
        "\n",
        "# Compare the accuracies\n",
        "if et_clf_tuned_score > et_clf_initial_score:\n",
        "    print(\"The accuracy of the tuned model is higher than the initial model.\")\n",
        "else:\n",
        "    print(\"The accuracy of the tuned model is lower than or equal to the initial model.\")\n"
      ],
      "metadata": {
        "id": "8jHq48Tclpwn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "importances = et_clf_tuned.feature_importances_\n",
        "features = X_train_combined.columns\n",
        "\n",
        "# Sort the features by their importance\n",
        "sorted_features = sorted(zip(importances, features), reverse=True)\n",
        "\n",
        "# Print the two most important features\n",
        "print(f\"The two most important features are: {sorted_features[0][1]} and {sorted_features[1][1]}\")\n"
      ],
      "metadata": {
        "id": "NOSPE6zHnQ4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yvOurBWcnz8L"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}